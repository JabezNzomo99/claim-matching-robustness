{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import jsonlines \n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set font globally to Helvetica Neue\n",
    "matplotlib.rcParams['font.family'] = 'Helvetica Neue'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_map_value(map_list, map_at):\n",
    "    \"\"\"\n",
    "    map_list is a list of dicts, like:\n",
    "      [\n",
    "        {\"map@1\": 0.8287},\n",
    "        {\"map@5\": 0.8823},\n",
    "        {\"map@10\": 0.8845},\n",
    "        {\"map@20\": 0.8857},\n",
    "        ...\n",
    "      ]\n",
    "    map_at is a string key, e.g. \"map@20\".\n",
    "    \"\"\"\n",
    "    for d in map_list:\n",
    "        if map_at in d:\n",
    "            return d[map_at]\n",
    "    # Return None or raise an error if the key doesn't exist\n",
    "    return None\n",
    "\n",
    "map_idx = {\n",
    "    \"map@1\": 0,\n",
    "    \"map@5\": 1,\n",
    "    \"map@10\": 2,\n",
    "    \"map@20\": 3,\n",
    "    \"map@50\": 4,\n",
    "}\n",
    "\n",
    "def get_map_value_from_list(map_list, map_at):\n",
    "    \"\"\"\n",
    "    Fetches the value from map_list at the position specified by map_at.\n",
    "\n",
    "    Parameters:\n",
    "    - map_list: List of map values (e.g., [0.92, 0.93, 0.94, ...]).\n",
    "    - map_at: String key indicating the desired map (e.g., \"map@20\").\n",
    "\n",
    "    Returns:\n",
    "    - The corresponding value from map_list if valid, else None.\n",
    "    \"\"\"\n",
    "    if map_at in map_idx:\n",
    "        idx = map_idx[map_at]\n",
    "        if 0 <= idx < len(map_list):\n",
    "            return map_list[idx]\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "# Dictionary mapping experiment types to file paths\n",
    "experiments_dict = {\n",
    "    \"Casing\": \"../experiments/casing/\",\n",
    "    \"Typos\": \"../experiments/typos/gpt4o/\", \n",
    "    \"Dialect\": \"../experiments/dialect/gpt4o/\",\n",
    "    \"Negation\": \"../experiments/negation/gpt4o/\",\n",
    "    \"Entity Replacement\": \"../experiments/named_entity_replacement/gpt4o/\",\n",
    "    \"Rewrite\": \"../experiments/rewrite/gpt4o/\",\n",
    "    \"Amplify/Downplay\": \"../experiments/amplify_minimize/gpt4o/\",\n",
    "}\n",
    "\n",
    "type_mapping = {\n",
    "    \"Entity Replacement\": {\n",
    "        \"Baseline\": \"Atleast 1\",\n",
    "        \"Worstcase\": \"All\"\n",
    "    },\n",
    "    \"Dialect\": {\n",
    "        \"Baseline\": \"Least\",\n",
    "        \"Worstcase\": \"Most\"\n",
    "    },\n",
    "    \"Typos\": {\n",
    "        \"Baseline\": \"Least\",\n",
    "        \"Worstcase\": \"Most\"\n",
    "    }, \n",
    "    \"Casing\": {\n",
    "        \"Baseline\": \"Truecase\",\n",
    "        \"Worstcase\": \"Upper\"\n",
    "    },\n",
    "    \"Rewrite\": {\n",
    "        \"Baseline\": \"Least\",\n",
    "        \"Worstcase\": \"Most\"\n",
    "    },\n",
    "    \"Amplify/Downplay\": {\n",
    "        \"Baseline\": \"Least\",\n",
    "        \"Worstcase\": \"Most\"\n",
    "    },\n",
    "    \"Negation\": {\n",
    "        \"Baseline\": \"Shallow\",\n",
    "        \"Worstcase\": \"Double\"\n",
    "    },\n",
    "}\n",
    "def parse_experiment_results(file_path: str, map_at:str = \"map@20\", after = False):\n",
    "    \"\"\"\n",
    "    Parses MAP@20 differences for a given experiment file.\n",
    "    \"\"\"\n",
    "    print('Trying to open', file_path)  \n",
    "    with open(file_path) as f:\n",
    "        results = [json.loads(line) for line in f]\n",
    "\n",
    "    parsed_results = {}\n",
    "    map_results = {}\n",
    "    for model_results in results:\n",
    "        model_name = next(iter(model_results))\n",
    "        \n",
    "        # Now use it in place of direct indexing\n",
    "        original_baseline_list = model_results[model_name]['original_baseline']['map_results']['test']\n",
    "        edited_baseline_list   = model_results[model_name]['edited_baseline']['map_results']['test']\n",
    "        original_worstcase_list = model_results[model_name]['original_worstcase']['map_results']['test']\n",
    "        edited_worstcase_list   = model_results[model_name]['edited_worstcase']['map_results']['test']\n",
    "\n",
    "        # if after use a difference map function\n",
    "        if after:\n",
    "            map_function = get_map_value_from_list\n",
    "        else:\n",
    "            map_function =  get_map_value\n",
    "\n",
    "        original_baseline   = map_function(original_baseline_list,   map_at)\n",
    "        edited_baseline     = map_function(edited_baseline_list,     map_at)\n",
    "        original_worstcase  = map_function(original_worstcase_list,  map_at)\n",
    "        edited_worstcase    = map_function(edited_worstcase_list,    map_at)\n",
    "\n",
    "        # Compute differences\n",
    "        baseline_diff = (edited_baseline - original_baseline) * 100\n",
    "        worstcase_diff = (edited_worstcase - original_worstcase) * 100\n",
    "\n",
    "        parsed_results[model_name] = {\"Baseline\": baseline_diff, \"Worstcase\": worstcase_diff}\n",
    "        map_results[model_name] = {\"Baseline_MAP\": edited_baseline, \"Worstcase_MAP\": edited_worstcase}\n",
    "\n",
    "    return pd.DataFrame(parsed_results).T, pd.DataFrame(map_results).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_experiments(experiments_dict, res_path, dataset, dialect_results_df = None, model_order=None, map_at=\"map@20\", after=False, plot_title:str=\"\"):\n",
    "    \"\"\"\n",
    "    Plots a heatmap with experiment names as group labels and types as sub-columns.\n",
    "\n",
    "    Args:\n",
    "        experiments_dict (dict): Dictionary mapping experiment names to file paths.\n",
    "        res_path (str): Relative path to the results file.\n",
    "        model_order (list): List of model names in the desired order (optional).\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib\n",
    "    import cmcrameri.cm as cmc\n",
    "    import numpy as np\n",
    "\n",
    "    # Set font globally to Helvetica Neue\n",
    "    matplotlib.rcParams['font.family'] = 'Helvetica Neue'\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    # Parse results for all experiments\n",
    "    for experiment, path in experiments_dict.items():\n",
    "        file_path = os.path.join(path, res_path)\n",
    "        experiment_df, _ = parse_experiment_results(file_path, map_at, after)\n",
    "\n",
    "        # Melt the data to long format and tag it with the experiment type\n",
    "        experiment_df = experiment_df.rename_axis(\"Model\").reset_index()\n",
    "        experiment_df = pd.melt(\n",
    "            experiment_df, id_vars=\"Model\", var_name=\"Type\", value_name=\"Difference\"\n",
    "        )\n",
    "        experiment_df[\"Experiment\"] = experiment\n",
    "        all_results.append(experiment_df)\n",
    "\n",
    "    # Combine all results\n",
    "    combined_df = pd.concat(all_results, ignore_index=True)\n",
    "\n",
    "     # Preserve the original 'Type' column for sorting\n",
    "    combined_df[\"OriginalType\"] = combined_df[\"Type\"]\n",
    "\n",
    "    # Replace Type labels using the type_mapping dictionary if provided\n",
    "    if type_mapping:\n",
    "        combined_df[\"Type\"] = combined_df.apply(\n",
    "            lambda row: type_mapping.get(row[\"Experiment\"], {}).get(row[\"Type\"], row[\"Type\"]),\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "    # Enforce Baseline -> Worstcase order using OriginalType\n",
    "    combined_df[\"OriginalType\"] = pd.Categorical(\n",
    "        combined_df[\"OriginalType\"],\n",
    "        categories=[\"Baseline\", \"Worstcase\"],\n",
    "        ordered=True\n",
    "    )\n",
    "\n",
    "    if dialect_results_df is not None: \n",
    "        combined_df = pd.concat([combined_df, dialect_results_df], ignore_index=True)\n",
    "\n",
    "\n",
    "    # Pivot table to create hierarchical columns (Experiment -> Type)\n",
    "    heatmap_df = combined_df.pivot_table(\n",
    "        index=\"Model\", columns=[\"Experiment\", \"Type\"], values=\"Difference\", sort=False\n",
    "    )\n",
    "\n",
    "    # Reorder rows (models) if specified\n",
    "    if model_order:\n",
    "        heatmap_df = heatmap_df.reindex(model_order)\n",
    "\n",
    "    # Prepare annotations\n",
    "    annot_data = heatmap_df.applymap(lambda x: f\"{x:.1f}%\" if not np.isnan(x) else \"\")\n",
    "\n",
    "    # Set up the heatmap\n",
    "    cmap = cmc.vik_r\n",
    "    fig, ax = plt.subplots(figsize=(14, 5))  # Adjust figure size\n",
    "    sns.heatmap(\n",
    "        heatmap_df,\n",
    "        cmap='RdBu',\n",
    "        annot=annot_data,\n",
    "        fmt=\"\",\n",
    "        center=0,\n",
    "        linewidths=0.5,\n",
    "        cbar_kws={\"orientation\": \"horizontal\", \"pad\": 0.05, \"shrink\":0.75},  # Tight color bar\n",
    "        ax=ax\n",
    "    )\n",
    "\n",
    "    cbar = ax.collections[0].colorbar\n",
    "    cbar.set_label(f\"{map_at} Difference after applying edits on {dataset} dataset and reranking\", fontsize=10, weight=\"bold\")\n",
    "\n",
    "    # Move the column headers to the top\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.xaxis.set_label_position(\"top\")\n",
    "    plt.xticks(rotation=0, ha=\"center\")\n",
    "\n",
    "    # Add experiment group labels (top-level annotations)\n",
    "    col_positions = heatmap_df.columns.codes[0]  # Top-level column indices\n",
    "    col_labels = heatmap_df.columns.levels[0]    # Unique experiment group labels\n",
    "    last_pos = 0\n",
    "\n",
    "    for i, label in enumerate(col_labels):\n",
    "        # Find how many columns belong to this group\n",
    "        group_count = list(col_positions).count(i)\n",
    "\n",
    "        # Calculate position for the group label\n",
    "        center = last_pos + group_count / 2\n",
    "        plt.text(\n",
    "            center, -1.4, label, ha=\"center\", va=\"center\", fontsize=11, fontweight=\"bold\"\n",
    "        )\n",
    "        last_pos += group_count\n",
    "\n",
    "        # Draw vertical line after each group\n",
    "        ax.axvline(last_pos, color=\"black\", linestyle=\"-\", linewidth=0.5)\n",
    "\n",
    "    # Add x-axis ticks (little lines) for the 'Type' labels\n",
    "    ax.xaxis.tick_top()  # Move ticks to the top\n",
    "    ax.xaxis.set_label_position(\"top\")\n",
    "    # ax.axhline(1, color=\"black\", linestyle=\"-\", linewidth=1)\n",
    "    # ax.axhline(5, color=\"black\", linestyle=\"-\", linewidth=1)\n",
    "    # ax.axhline(8, color=\"black\", linestyle=\"-\", linewidth=1)\n",
    "    # Set x-axis tick positions and labels\n",
    "    ax.set_xticks([pos + 0.5 for pos in range(len(heatmap_df.columns))])  # Center ticks\n",
    "    ax.set_xticklabels(\n",
    "        [col[1] for col in heatmap_df.columns],  # Extract 'Type' labels\n",
    "        rotation=0,\n",
    "        ha=\"center\",\n",
    "        fontsize=10\n",
    "    )\n",
    "\n",
    "    # Enable tick marks for the x-axis\n",
    "    ax.tick_params(\n",
    "        axis=\"x\",      # Apply to the x-axis\n",
    "        which=\"both\",  # Enable both major and minor ticks\n",
    "        length=3.5,      # Length of the tick lines\n",
    "        width=1,       # Width of the tick lines\n",
    "        direction=\"out\"  # Draw ticks outward\n",
    "    )\n",
    "\n",
    "    # Clear axis labels\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"\")\n",
    "    ax.tick_params(axis='y',length=0)  # Remove tick marks for clarity\n",
    "\n",
    "    # Add a border around the heatmap\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(True)\n",
    "        spine.set_linewidth(1.0)\n",
    "\n",
    "    # Adjust layout\n",
    "    fig.tight_layout(pad=1.0)\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    plt.savefig(f'{plot_title}_{dataset}_plot.png', dpi=600, bbox_inches=\"tight\")  # Save with high DPI\n",
    "    plt.show()\n",
    "\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_map_differences(file_path: str, map_at: str = \"map@20\", after=False):\n",
    "    \"\"\"\n",
    "    Calculates the MAP@20 differences for each dialect and model.\n",
    "    Args:\n",
    "        file_path: Path to the JSON file containing results.\n",
    "    Returns:\n",
    "        A pandas DataFrame with models as rows, dialects as columns, and MAP@20 differences as values.\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    results = []\n",
    "    print('Trying to open', file_path)\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            results.append(json.loads(line))\n",
    "\n",
    "    # Initialize a dictionary to store differences\n",
    "    differences = {}\n",
    "    map_results = {}\n",
    "    for entry in results:\n",
    "        dialect = entry[\"dialect\"]\n",
    "        for model, model_results in entry.items():\n",
    "            if model == \"dialect\":  # Skip dialect key\n",
    "                continue\n",
    "            \n",
    "            # Usage example:\n",
    "            original_baseline_list = model_results[\"original_baseline\"][\"map_results\"][\"test\"]\n",
    "            edited_baseline_list   = model_results[\"edited_baseline\"][\"map_results\"][\"test\"]\n",
    "\n",
    "            # if after use a difference map function\n",
    "            if after:\n",
    "                map_function = get_map_value_from_list\n",
    "            else:\n",
    "                map_function =  get_map_value\n",
    "\n",
    "            original_baseline = map_function(original_baseline_list, map_at)\n",
    "            edited_baseline   = map_function(edited_baseline_list, map_at)\n",
    "\n",
    "            # Compute the difference\n",
    "            map_diff = (edited_baseline - original_baseline) * 100  # Scale to percentage\n",
    "            \n",
    "            # Add to dictionary\n",
    "            if model not in differences:\n",
    "                differences[model] = {}\n",
    "            differences[model][dialect] = round(map_diff, 2)\n",
    "\n",
    "            # Add to map_results dictionary\n",
    "            if model not in map_results:\n",
    "                map_results[model] = {}\n",
    "            if dialect not in map_results[model]:\n",
    "                map_results[model][dialect] = {}\n",
    "\n",
    "            map_results[model][dialect] = {\n",
    "            'Worstcase_MAP': edited_baseline\n",
    "            }\n",
    "\n",
    "    # Convert to a pandas DataFrame\n",
    "    df = pd.DataFrame(differences).T  # Transpose so models are rows, dialects are columns\n",
    "    df = df.fillna(0)  # Fill missing values with 0\n",
    "    # print(df)\n",
    "\n",
    "    # Flatten the map_results dictionary\n",
    "    map_results_flat = [\n",
    "        {\"Model\": model, \"Dialect\": dialect, **metrics}\n",
    "        for model, dialect_data in map_results.items()\n",
    "        for dialect, metrics in dialect_data.items()\n",
    "    ]\n",
    "    map_results_df = pd.DataFrame(map_results_flat)\n",
    "\n",
    "    return df, map_results_df\n",
    "\n",
    "def parse_dialect_results(file_path: str, map_at: str = \"map@20\", after=False):\n",
    "    \"\"\"\n",
    "    Parses MAP@20 differences for a given dialect file.\n",
    "    \"\"\"\n",
    "    # File path to the JSON file\n",
    "    map_differences_df, map_results_df = calculate_map_differences(file_path, map_at, after)\n",
    "    map_differences_df = map_differences_df.reset_index().rename(columns={\"index\": \"Model\"})\n",
    "    dialect_check_that_df = pd.melt(\n",
    "        map_differences_df,\n",
    "        id_vars=[\"Model\"],\n",
    "        var_name=\"Type\",         # Column names become 'Type'\n",
    "        value_name=\"Difference\"  # Values become 'Difference'\n",
    "    )\n",
    "\n",
    "    # Add Experiment and OriginalType columns\n",
    "    dialect_check_that_df[\"Experiment\"] = \"Dialect\"\n",
    "    dialect_check_that_df[\"OriginalType\"] = \"Worstcase\"\n",
    "\n",
    "    # Mapping dictionary to shorten dialect names\n",
    "    type_mapping_dialects = {\n",
    "        \"african_american_english\": \"AAE\",\n",
    "        \"jamaican_patois\": \"Jamaican\",\n",
    "        \"pidgin\": \"Pidgin\",\n",
    "        \"singlish\": \"Singlish\"\n",
    "    }\n",
    "\n",
    "    # Remap the 'Type' column\n",
    "    dialect_check_that_df[\"Type\"] = dialect_check_that_df[\"Type\"].replace(type_mapping_dialects)\n",
    "    map_results_df[\"Dialect\"] = map_results_df[\"Dialect\"].replace(type_mapping_dialects)\n",
    "\n",
    "    return dialect_check_that_df, map_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dialect experiments have a different structure, so we need to parse them separately\n",
    "file_path = \"../experiments/dialect/gpt4o/clef2021-checkthat-task2a--english/results/before_reranking_dialect_results_all.jsonl\"\n",
    "# dialect_check_that_df_map1, map1 = parse_dialect_results(file_path, map_at=\"map@1\")\n",
    "# dialect_check_that_df_map10 = parse_dialect_results(file_path, map_at=\"map@10\")\n",
    "dialect_check_that_df_map20, map20_results = parse_dialect_results(file_path, map_at=\"map@50\")\n",
    "# dialect_check_that_df_map50 = parse_dialect_results(file_path, map_at=\"map@50\")\n",
    "# dialect_check_that_df_map100 = parse_dialect_results(file_path, map_at=\"map@100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the dialect results for the Fact-Check-Tweet dataset\n",
    "fc_file_path = \"../experiments/dialect/gpt4o/fact-check-tweet/results/before_reranking_dialect_results_all_final.jsonl\"\n",
    "dialect_fact_check_tweet_df_map20, fact_check_tweet_map20_results = parse_dialect_results(fc_file_path, map_at=\"map@20\", after=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify custom order for models\n",
    "custom_model_order = [\n",
    "    \"bm25\",\n",
    "    \"all-distilroberta-v1\",\n",
    "    \"all-MiniLM-L12-v2\",\n",
    "    \"all-mpnet-base-v2\",\n",
    "    \"all-mpnet-base-v2-ft\",\n",
    "    \"sentence-t5-base\",\n",
    "    \"sentence-t5-large\",\n",
    "    \"sentence-t5-large-ft\",\n",
    "    \"hkunlp/instructor-base\",\n",
    "    \"hkunlp/instructor-large\",\n",
    "    \"Salesforce/SFR-Embedding-Mistral\",\n",
    "    \"nvidia/NV-Embed-v2\"\n",
    "]\n",
    "\n",
    "# map_ats = [\"map@1\", \"map@10\", \"map@20\", \"map@50\", \"map@100\"]\n",
    "# dialect_results_dfs = [dialect_check_that_df_map1, dialect_check_that_df_map10, dialect_check_that_df_map20, dialect_check_that_df_map50, dialect_check_that_df_map100]\n",
    "\n",
    "# for map_at, dialect_results_df in zip(map_ats, dialect_results_dfs):\n",
    "#     print(f\"Generating plot for {map_at}\")\n",
    "#     results_df = plot_all_experiments(\n",
    "#         experiments_dict=experiments_dict,\n",
    "#         res_path='clef2021-checkthat-task2a--english/results/before_reranking_results_all.jsonl',\n",
    "#         dataset=\"CheckThat2022\",\n",
    "#         dialect_results_df=dialect_results_df,\n",
    "#         model_order=custom_model_order,\n",
    "#         map_at=map_at\n",
    "#     )\n",
    "\n",
    "# # # Run the function with custom model order\n",
    "# # results_df = plot_all_experiments(experiments_dict, model_order=custom_model_order, res_path ='clef2021-checkthat-task2a--english/results/before_reranking_results_all.jsonl', map_at = \"map@10\", dataset=\"CheckThat2022\", dialect_results_df=dialect_check_that_df_map10)\n",
    "# # results_df = plot_all_experiments(experiments_dict, model_order=custom_model_order, res_path ='clef2021-checkthat-task2a--english/results/before_reranking_results_all.jsonl', map_at = \"map@20\", dataset=\"CheckThat2022\", dialect_results_df=dialect_check_that_df_map20)\n",
    "# # results_df = plot_all_experiments(experiments_dict, model_order=custom_model_order, res_path ='clef2021-checkthat-task2a--english/results/before_reranking_results_all.jsonl', map_at = \"map@50\", dataset=\"CheckThat2022\", dialect_results_df=dialect_check_that_df_map50)\n",
    "# # results_df = plot_all_experiments(experiments_dict, model_order=custom_model_order, res_path ='clef2021-checkthat-task2a--english/results/before_reranking_results_all.jsonl', map_at = \"map@100\", dataset=\"CheckThat2022\", dialect_results_df=dialect_check_that_df_map100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_that_before_ranking_results_df = plot_all_experiments(experiments_dict,\n",
    "                                model_order=custom_model_order, \n",
    "                                res_path ='clef2021-checkthat-task2a--english/results/before_reranking_results_all.jsonl', \n",
    "                                map_at = \"map@50\", dataset=\"CheckThat2022\", \n",
    "                                dialect_results_df=dialect_check_that_df_map20,\n",
    "                                plot_title=\"before_reranking_results_all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_check_results_before_ranking = plot_all_experiments(experiments_dict,\n",
    "                                        model_order=custom_model_order, \n",
    "                                        res_path ='fact-check-tweet/results/before_rerank_results_all.jsonl', \n",
    "                                        map_at = \"map@20\", dataset=\"FactCheckTweet\", \n",
    "                                        # after=True,\n",
    "                                        dialect_results_df=dialect_fact_check_tweet_df_map20,\n",
    "                                        plot_title=\"before_reranking_results_all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_idx = {\n",
    "    \"map@1\": 0,\n",
    "    \"map@5\": 1,\n",
    "    \"map@10\": 2,\n",
    "    \"map@20\": 3,\n",
    "    \"map@50\": 4,\n",
    "}\n",
    "\n",
    "def get_map_value_from_list(map_list, map_at):\n",
    "    \"\"\"\n",
    "    Fetches the value from map_list at the position specified by map_at.\n",
    "\n",
    "    Parameters:\n",
    "    - map_list: List of map values (e.g., [0.92, 0.93, 0.94, ...]).\n",
    "    - map_at: String key indicating the desired map (e.g., \"map@20\").\n",
    "\n",
    "    Returns:\n",
    "    - The corresponding value from map_list if valid, else None.\n",
    "    \"\"\"\n",
    "    if map_at in map_idx:\n",
    "        idx = map_idx[map_at]\n",
    "        if 0 <= idx < len(map_list):\n",
    "            return map_list[idx]\n",
    "    elif map_at == \"map@50\":\n",
    "        return map_list[-1]\n",
    "    return None\n",
    "\n",
    "\n",
    "def parse_experiment_results_after_ranking(file_path: str, map_at:str = \"map@20\"):\n",
    "    \"\"\"\n",
    "    Parses MAP@20 differences for a given experiment file.\n",
    "    \"\"\"\n",
    "    print('Trying to open', file_path)  \n",
    "    with open(file_path) as f:\n",
    "        results = [json.loads(line) for line in f]\n",
    "\n",
    "    parsed_results = {}\n",
    "    map_results = {}\n",
    "    for model_results in results:\n",
    "        model_name = next(iter(model_results))\n",
    "        \n",
    "        # Now use it in place of direct indexing\n",
    "        original_baseline_list = model_results[model_name]['original_baseline']['map_results']['test']\n",
    "        edited_baseline_list   = model_results[model_name]['edited_baseline']['map_results']['test']\n",
    "        original_worstcase_list = model_results[model_name]['original_worstcase']['map_results']['test']\n",
    "        edited_worstcase_list   = model_results[model_name]['edited_worstcase']['map_results']['test']\n",
    "\n",
    "        original_baseline   = get_map_value_from_list(original_baseline_list,   map_at)\n",
    "        edited_baseline     = get_map_value_from_list(edited_baseline_list,     map_at)\n",
    "        original_worstcase  = get_map_value_from_list(original_worstcase_list,  map_at)\n",
    "        edited_worstcase    = get_map_value_from_list(edited_worstcase_list,    map_at)\n",
    "\n",
    "        # Compute differences\n",
    "        baseline_diff = (edited_baseline - original_baseline) * 100\n",
    "        worstcase_diff = (edited_worstcase - original_worstcase) * 100\n",
    "\n",
    "        parsed_results[model_name] = {\"Baseline\": baseline_diff, \n",
    "                                      \"Worstcase\": worstcase_diff}\n",
    "        \n",
    "        map_results[model_name] = {\"Baseline_MAP\": edited_baseline, \"Worstcase_MAP\": edited_worstcase}\n",
    "\n",
    "    return pd.DataFrame(parsed_results).T, pd.DataFrame(map_results).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def calculate_dialect_map_differences(file_path: str, dialect:str, map_at: str = \"map@20\"):\n",
    "    \"\"\"\n",
    "    Calculates the MAP@20 differences for each dialect and model.\n",
    "    Args:\n",
    "        file_path: Path to the JSON file containing results.\n",
    "    Returns:\n",
    "        A pandas DataFrame with models as rows, dialects as columns, and MAP@20 differences as values.\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    results = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            results.append(json.loads(line))\n",
    "\n",
    "    # Initialize a dictionary to store differences and results\n",
    "    differences = {}\n",
    "    map_results = {}\n",
    "\n",
    "    for model_results in results:\n",
    "        model = next(iter(model_results))\n",
    "\n",
    "        print(model_results[model])\n",
    "        \n",
    "        # Extract original and edited MAP values\n",
    "        original_baseline_list = model_results[model][\"original_baseline\"][\"map_results\"][\"test\"]\n",
    "        edited_baseline_list = model_results[model][\"edited_baseline\"][\"map_results\"][\"test\"]\n",
    "\n",
    "        print(f'Original Baseline List: {original_baseline_list}')\n",
    "        print(f'Edited Baseline List: {edited_baseline_list}')\n",
    "\n",
    "        original_baseline = get_map_value_from_list(original_baseline_list, map_at)\n",
    "        edited_baseline = get_map_value_from_list(edited_baseline_list, map_at)\n",
    "\n",
    "        print(f'Original Baseline: {original_baseline}')\n",
    "        print(f'Edited Baseline: {edited_baseline}')\n",
    "\n",
    "        # Compute the difference\n",
    "        map_diff = (edited_baseline - original_baseline) * 100  # Scale to percentage\n",
    "\n",
    "        # Add to dictionaries\n",
    "        if model not in differences:\n",
    "            differences[model] = {}\n",
    "            map_results[model] = {}\n",
    "        differences[model][dialect] = round(map_diff, 2)\n",
    "        map_results[model][dialect] = {\n",
    "            \"Baseline_MAP\": original_baseline,\n",
    "            \"Worstcase_MAP\": edited_baseline,\n",
    "        }\n",
    "\n",
    "    # Convert to DataFrames\n",
    "    df = pd.DataFrame(differences).T  # Transpose so models are rows, dialects are columns\n",
    "    df = df.fillna(0)  # Fill missing values with 0\n",
    "\n",
    "    map_results_flat = [\n",
    "        {\"Model\": model, \"Dialect\": dialect, **metrics}\n",
    "        for model, dialect_data in map_results.items()\n",
    "        for dialect, metrics in dialect_data.items()\n",
    "    ]\n",
    "    map_df = pd.DataFrame(map_results_flat)\n",
    "\n",
    "    return df, map_df\n",
    "\n",
    "def parse_dialect_results(file_path: str, map_at: str = \"map@20\"):\n",
    "    \"\"\"\n",
    "    Parses MAP@20 differences for a given dialect file.\n",
    "    \"\"\"\n",
    "    # File path to the JSON file\n",
    "    map_differences_df, map_results_df = calculate_map_differences(file_path, map_at)\n",
    "    map_differences_df = map_differences_df.reset_index().rename(columns={\"index\": \"Model\"})\n",
    "    dialect_check_that_df = pd.melt(\n",
    "        map_differences_df,\n",
    "        id_vars=[\"Model\"],\n",
    "        var_name=\"Type\",         # Column names become 'Type'\n",
    "        value_name=\"Difference\"  # Values become 'Difference'\n",
    "    )\n",
    "\n",
    "    # Add Experiment and OriginalType columns\n",
    "    dialect_check_that_df[\"Experiment\"] = \"Dialect\"\n",
    "    dialect_check_that_df[\"OriginalType\"] = \"Worstcase\"\n",
    "\n",
    "    # Mapping dictionary to shorten dialect names\n",
    "    type_mapping_dialects = {\n",
    "        \"african_american_english\": \"AAE\",\n",
    "        \"jamaican_patois\": \"Jamaican\",\n",
    "        \"pidgin\": \"Pidgin\",\n",
    "        \"singlish\": \"Singlish\"\n",
    "    }\n",
    "\n",
    "    # Remap the 'Type' column\n",
    "    dialect_check_that_df[\"Type\"] = dialect_check_that_df[\"Type\"].replace(type_mapping_dialects)\n",
    "    map_results_df[\"Dialect\"] = map_results_df[\"Dialect\"].replace(type_mapping_dialects)\n",
    "\n",
    "    return dialect_check_that_df, map_results_df\n",
    "\n",
    "\n",
    "def parse_dialect_results_after(dialect_paths: dict, dataset:str, map_at: str = \"map@20\", map_results_before_df=None):\n",
    "    \"\"\"\n",
    "    Parses MAP@20 differences for a given dialect file.\n",
    "    \"\"\"\n",
    "    all_dialect_results = []\n",
    "    all_dialecct_e2e_results = []\n",
    "    for dialect, dialect_path in dialect_paths.items():\n",
    "        file_path = f'../experiments/dialect/gpt4o/{dataset}/results/{dialect_path}/after_reranking_n_50_candidates_bge_llm_results.jsonl'\n",
    "\n",
    "        print(f\"Processing {file_path} for {dialect} dialect.\")\n",
    "        # Calculate differences and map results\n",
    "        map_differences_df, map_results_after_df = calculate_dialect_map_differences(file_path, dialect, map_at)\n",
    "\n",
    "        # Ensure map_results_before_df is provided\n",
    "        if map_results_before_df is None:\n",
    "            raise ValueError(\"map_results_before_df is required to calculate differences.\")\n",
    "        \n",
    "        # print(map_results_before_df)\n",
    "        # Merge and calculate differences\n",
    "        merged_df = map_results_after_df.merge(\n",
    "            map_results_before_df, on=[\"Model\", \"Dialect\"], suffixes=(\"_after\", \"_before\")\n",
    "        )\n",
    "\n",
    "        # print(merged_df)\n",
    "        merged_df[\"Difference\"] = (merged_df[\"Worstcase_MAP_after\"] - merged_df[\"Worstcase_MAP_before\"]) * 100\n",
    "\n",
    "        # Melt the data to long format\n",
    "        filtered_df = merged_df[[\"Model\", \"Dialect\", \"Difference\"]]\n",
    "        # Rename difference to dialect\n",
    "        filtered_df = filtered_df.rename(columns={\"Dialect\": 'Type'})\n",
    "        # Add Experiment and OriginalType columns\n",
    "        filtered_df[\"Experiment\"] = \"Dialect\"\n",
    "        filtered_df[\"OriginalType\"] = \"Worstcase\"\n",
    "\n",
    "        all_dialect_results.append(filtered_df)\n",
    "\n",
    "        map_differences_df = map_differences_df.reset_index().rename(columns={\"index\": \"Model\"})\n",
    "        dialect_check_that_df = pd.melt(\n",
    "            map_differences_df,\n",
    "            id_vars=[\"Model\"],\n",
    "            var_name=\"Type\",         # Column names become 'Type'\n",
    "            value_name=\"Difference\"  # Values become 'Difference'\n",
    "        )\n",
    "\n",
    "        # Add Experiment and OriginalType columns\n",
    "        dialect_check_that_df[\"Experiment\"] = \"Dialect\"\n",
    "        dialect_check_that_df[\"OriginalType\"] = \"Worstcase\"\n",
    "        all_dialecct_e2e_results.append(dialect_check_that_df)\n",
    "\n",
    "    return pd.concat(all_dialect_results, ignore_index=True), pd.concat(all_dialecct_e2e_results, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialect_paths = {\n",
    "    'AAE':'dialect_aae',\n",
    "    'Jamaican':'dialect_patois',\n",
    "    'Pidgin':'dialect_pidgin',\n",
    "    'Singlish':'dialect_singlish',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'clef2021-checkthat-task2a--english'\n",
    "dialect_results_after, e2e_dialect_map20 = parse_dialect_results_after(dialect_paths, dataset, map_at=\"map@50\", map_results_before_df=map20_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'fact-check-tweet'\n",
    "dialect_results_after_fact_check, e2e_dialect_map20_fact_check = parse_dialect_results_after(dialect_paths, dataset, map_at=\"map@20\", map_results_before_df=fact_check_tweet_map20_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the effect of reranking on MAP@20\n",
    "# Does reranking actually help?\n",
    "# Load the after reranking results\n",
    "def plot_all_experiments_after(experiments_dict, \n",
    "                               res_path, \n",
    "                               after_res_path, \n",
    "                               dataset, \n",
    "                               dialect_results_df = None,\n",
    "                               model_order=None, \n",
    "                               map_at=\"map@20\", \n",
    "                               reranker='Bge-reranker-v2-gemma',\n",
    "                               n_candidates=50,\n",
    "                               title=\"recovery_after_reranking_results_all\"):\n",
    "    \"\"\"\n",
    "    Plots a heatmap with experiment names as group labels and types as sub-columns.\n",
    "\n",
    "    Args:\n",
    "        experiments_dict (dict): Dictionary mapping experiment names to file paths.\n",
    "        res_path (str): Relative path to the results file.\n",
    "        model_order (list): List of model names in the desired order (optional).\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib\n",
    "    import cmcrameri.cm as cmc\n",
    "    import numpy as np\n",
    "\n",
    "    # Set font globally to Helvetica Neue\n",
    "    matplotlib.rcParams['font.family'] = 'Helvetica Neue'\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    # Parse results for all experiments\n",
    "    for experiment, path in experiments_dict.items():\n",
    "        before_reranking_path = os.path.join(path, res_path)\n",
    "        after_reranking_path = os.path.join(path, after_res_path) \n",
    "        _, map_results_before_df = parse_experiment_results(before_reranking_path, map_at=map_at)\n",
    "        _, map_results_after_df = parse_experiment_results_after_ranking(after_reranking_path, map_at=map_at)\n",
    "\n",
    "        #  Calculate the difference in MAP@20 after reranking\n",
    "        map_results_after_df[\"Baseline\"] = (map_results_after_df[\"Baseline_MAP\"] - map_results_before_df[\"Baseline_MAP\"]) * 100\n",
    "        map_results_after_df[\"Worstcase\"] = (map_results_after_df[\"Worstcase_MAP\"] - map_results_before_df[\"Worstcase_MAP\"])*100\n",
    "\n",
    "        # Melt the data to long format and tag it with the experiment type\n",
    "        filtered_df = map_results_after_df[['Baseline', 'Worstcase']]\n",
    "        experiment_df = filtered_df.rename_axis(\"Model\").reset_index()\n",
    "        experiment_df = pd.melt(\n",
    "            experiment_df, id_vars=\"Model\", var_name=\"Type\", value_name=\"Difference\"\n",
    "        )\n",
    "        experiment_df[\"Experiment\"] = experiment\n",
    "        all_results.append(experiment_df)\n",
    "\n",
    "    # Combine all results\n",
    "    combined_df = pd.concat(all_results, ignore_index=True)\n",
    "\n",
    "    print(\"We are here .. \")\n",
    "    print(combined_df.shape)\n",
    "\n",
    "     # Preserve the original 'Type' column for sorting\n",
    "    combined_df[\"OriginalType\"] = combined_df[\"Type\"]\n",
    "\n",
    "    # Replace Type labels using the type_mapping dictionary if provided\n",
    "    if type_mapping:\n",
    "        combined_df[\"Type\"] = combined_df.apply(\n",
    "            lambda row: type_mapping.get(row[\"Experiment\"], {}).get(row[\"Type\"], row[\"Type\"]),\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "    # Enforce Baseline -> Worstcase order using OriginalType\n",
    "    combined_df[\"OriginalType\"] = pd.Categorical(\n",
    "        combined_df[\"OriginalType\"],\n",
    "        categories=[\"Baseline\", \"Worstcase\"],\n",
    "        ordered=True\n",
    "    )\n",
    "\n",
    "    if dialect_results_df is not None: \n",
    "        combined_df = pd.concat([combined_df, dialect_results_df], ignore_index=True)\n",
    "\n",
    "\n",
    "    # Pivot table to create hierarchical columns (Experiment -> Type)\n",
    "    heatmap_df = combined_df.pivot_table(\n",
    "        index=\"Model\", columns=[\"Experiment\", \"Type\"], values=\"Difference\", sort=False\n",
    "    )\n",
    "\n",
    "    # Reorder rows (models) if specified\n",
    "    if model_order:\n",
    "        heatmap_df = heatmap_df.reindex(model_order)\n",
    "\n",
    "    # Prepare annotations\n",
    "    annot_data = heatmap_df.applymap(\n",
    "        lambda x: f\"+{x:.1f}%\" if x > 0 else (f\"{x:.1f}%\" if x < 0 else \"\") if not np.isnan(x) else \"\"\n",
    "    )\n",
    "\n",
    "    # Set up the heatmap\n",
    "    cmap = cmc.vik_r\n",
    "    fig, ax = plt.subplots(figsize=(14, 5))  # Adjust figure size\n",
    "    sns.heatmap(\n",
    "        heatmap_df,\n",
    "        cmap='RdYlGn',\n",
    "        annot=annot_data,\n",
    "        fmt=\"\",\n",
    "        center=0.2,\n",
    "        linewidths=0.5,\n",
    "        cbar_kws={\"orientation\": \"horizontal\", \"pad\": 0.05, \"shrink\":0.75},  # Tight color bar\n",
    "        ax=ax\n",
    "    )\n",
    "\n",
    "    cbar = ax.collections[0].colorbar\n",
    "    cbar.set_label(f\"{map_at} improvement after reranking {n_candidates} candidates using {reranker} on {dataset} dataset\", fontsize=10, weight=\"bold\")\n",
    "\n",
    "    # Move the column headers to the top\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.xaxis.set_label_position(\"top\")\n",
    "    plt.xticks(rotation=0, ha=\"center\")\n",
    "\n",
    "    # Add experiment group labels (top-level annotations)\n",
    "    col_positions = heatmap_df.columns.codes[0]  # Top-level column indices\n",
    "    col_labels = heatmap_df.columns.levels[0]    # Unique experiment group labels\n",
    "    last_pos = 0\n",
    "\n",
    "    for i, label in enumerate(col_labels):\n",
    "        # Find how many columns belong to this group\n",
    "        group_count = list(col_positions).count(i)\n",
    "\n",
    "        # Calculate position for the group label\n",
    "        center = last_pos + group_count / 2\n",
    "        plt.text(\n",
    "            center, -1.4, label, ha=\"center\", va=\"center\", fontsize=11, fontweight=\"bold\"\n",
    "        )\n",
    "        last_pos += group_count\n",
    "\n",
    "        # Draw vertical line after each group\n",
    "        ax.axvline(last_pos, color=\"black\", linestyle=\"-\", linewidth=0.5)\n",
    "\n",
    "    # Add x-axis ticks (little lines) for the 'Type' labels\n",
    "    ax.xaxis.tick_top()  # Move ticks to the top\n",
    "    ax.xaxis.set_label_position(\"top\")\n",
    "    # ax.axhline(1, color=\"black\", linestyle=\"-\", linewidth=1)\n",
    "    # ax.axhline(5, color=\"black\", linestyle=\"-\", linewidth=1)\n",
    "    # ax.axhline(8, color=\"black\", linestyle=\"-\", linewidth=1)\n",
    "    # Set x-axis tick positions and labels\n",
    "    ax.set_xticks([pos + 0.5 for pos in range(len(heatmap_df.columns))])  # Center ticks\n",
    "    ax.set_xticklabels(\n",
    "        [col[1] for col in heatmap_df.columns],  # Extract 'Type' labels\n",
    "        rotation=0,\n",
    "        ha=\"center\",\n",
    "        fontsize=10\n",
    "    )\n",
    "\n",
    "    # Enable tick marks for the x-axis\n",
    "    ax.tick_params(\n",
    "        axis=\"x\",      # Apply to the x-axis\n",
    "        which=\"both\",  # Enable both major and minor ticks\n",
    "        length=3.5,      # Length of the tick lines\n",
    "        width=1,       # Width of the tick lines\n",
    "        direction=\"out\"  # Draw ticks outward\n",
    "    )\n",
    "\n",
    "    # Clear axis labels\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"\")\n",
    "    ax.tick_params(axis='y',length=0)  # Remove tick marks for clarity\n",
    "\n",
    "    # Add a border around the heatmap\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(True)\n",
    "        spine.set_linewidth(1.0)\n",
    "\n",
    "    # Adjust layout\n",
    "    fig.tight_layout(pad=1.0)\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    plt.savefig(f'{title}_{dataset}_plot.png', dpi=600, bbox_inches=\"tight\")  # Save with high DPI\n",
    "    plt.show()\n",
    "\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary mapping experiment types to file paths\n",
    "experiments_dict = {\n",
    "    \"Casing\": \"../experiments/casing/\",\n",
    "    \"Typos\": \"../experiments/typos/gpt4o/\", \n",
    "    # \"Dialect\": \"../experiments/dialect/gpt4o/\",\n",
    "    \"Negation\": \"../experiments/negation/gpt4o/\",\n",
    "    \"Entity Replacement\": \"../experiments/named_entity_replacement/gpt4o/\",\n",
    "    \"Rewrite\": \"../experiments/rewrite/gpt4o/\",\n",
    "    # \"Amplify/Downplay\": \"../experiments/amplify_minimize/gpt4o/\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recovery_check_that_df = plot_all_experiments_after(experiments_dict,\n",
    "                                  model_order=custom_model_order, \n",
    "                                  res_path ='clef2021-checkthat-task2a--english/results/before_reranking_results_all.jsonl', \n",
    "                                  after_res_path = 'clef2021-checkthat-task2a--english/results/after_reranking_n_50_candidates_bge_llm_results.jsonl',\n",
    "                                  map_at = \"map@50\", \n",
    "                                  dataset=\"CheckThat2022\",\n",
    "                                  dialect_results_df=dialect_results_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recovery_fact_check_tweet_df = plot_all_experiments_after(experiments_dict,\n",
    "                                  model_order=custom_model_order, \n",
    "                                  res_path ='fact-check-tweet/results/before_rerank_results_all.jsonl', \n",
    "                                  after_res_path = 'fact-check-tweet/results/after_reranking_n_50_candidates_bge_llm_results.jsonl',\n",
    "                                  map_at = \"map@20\", \n",
    "                                  dataset=\"Fact Check Tweet\",\n",
    "                                  dialect_results_df=dialect_results_after_fact_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e2e comparison\n",
    "e2e_results_check_that_df = plot_all_experiments(experiments_dict, model_order=custom_model_order, res_path ='clef2021-checkthat-task2a--english/results/after_reranking_n_50_candidates_bge_llm_results.jsonl', map_at = \"map@20\", dataset=\"CheckThat2022\", \n",
    "                                  after=True,\n",
    "                                  dialect_results_df=e2e_dialect_map20,\n",
    "                                  plot_title=\"after_reranking_results_all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e2e comparison\n",
    "fact_check_tweet_e2e_results_df = plot_all_experiments(\n",
    "                                  experiments_dict, \n",
    "                                  model_order=custom_model_order, \n",
    "                                  res_path ='fact-check-tweet/results/after_reranking_n_50_candidates_bge_llm_results.jsonl', map_at = \"map@20\", dataset=\"CheckThat2022\", \n",
    "                                  after=True,\n",
    "                                  dialect_results_df=e2e_dialect_map20_fact_check,\n",
    "                                  plot_title=\"after_reranking_results_all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the dataframes for fact-check-tweet\n",
    "# fact_check_results_before_ranking\n",
    "# recovery_fact_check_tweet_df\n",
    "# fact_check_tweet_e2e_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Util function to generate latex gradient table for an experiment\n",
    "\n",
    "# Arrange models in the desired order\n",
    "custom_model_order = [\n",
    "    \"bm25\",\n",
    "    \"all-distilroberta-v1\",\n",
    "    \"all-MiniLM-L12-v2\",\n",
    "    \"all-mpnet-base-v2\",\n",
    "    \"all-mpnet-base-v2-ft\",\n",
    "    \"sentence-t5-base\",\n",
    "    \"sentence-t5-large\",\n",
    "    \"sentence-t5-large-ft\",\n",
    "    \"hkunlp/instructor-base\",\n",
    "    \"hkunlp/instructor-large\",\n",
    "    \"Salesforce/SFR-Embedding-Mistral\",\n",
    "    \"nvidia/NV-Embed-v2\"\n",
    "]\n",
    "\n",
    "# Dict containing name to be displayed in the latex table\n",
    "model_name_mapping = {\n",
    "    \"bm25\": \"BM25\",\n",
    "    \"all-distilroberta-v1\": \"all-distilroberta-v1\",\n",
    "    \"all-MiniLM-L12-v2\": \"all-MiniLM-L12-v2\",\n",
    "    \"all-mpnet-base-v2\": \"all-mpnet-base-v2\",\n",
    "    \"all-mpnet-base-v2-ft\": \"all-mpnet-base-v2-ft\",\n",
    "    \"sentence-t5-base\": \"sentence-t5-base\",\n",
    "    \"sentence-t5-large\": \"sentence-t5-large\",\n",
    "    \"sentence-t5-large-ft\": \"sentence-t5-large-ft\",\n",
    "    \"hkunlp/instructor-base\": \"instructor-base\",\n",
    "    \"hkunlp/instructor-large\": \"instructor-large\",\n",
    "    \"Salesforce/SFR-Embedding-Mistral\": \"SFR-Embedding-Mistral\",\n",
    "    \"nvidia/NV-Embed-v2\": \"NV-Embed-v2\"\n",
    "}\n",
    "\n",
    "def generate_latex(results_df:pd.DataFrame, \n",
    "                   exclude_experiments:list = ['Amplify/Downplay'], \n",
    "                   custom_model_order:list = custom_model_order,\n",
    "                   model_name_mapping:dict = model_name_mapping,\n",
    "                   vmin:float = None,\n",
    "                   vmax:float = None, \n",
    "                   pos_color_code = \"pos\", \n",
    "                   neg_color_code = \"neg\"):   \n",
    "    # Filter out the excluded experiments\n",
    "    filtered_df = results_df[~results_df['Experiment'].isin(exclude_experiments)]\n",
    "\n",
    "    # Pivot table to create hierarchical columns (Experiment -> Type)\n",
    "    heatmap_df = filtered_df.pivot_table(\n",
    "        index=\"Model\", columns=[\"Experiment\", \"Type\"], values=\"Difference\", sort=False\n",
    "    )\n",
    "\n",
    "    # Arrange heatmap models in the desired order\n",
    "    heatmap_df = heatmap_df.reindex(custom_model_order) \n",
    "\n",
    "    # Get the values of the heatmap as a numpy array\n",
    "    heatmap_values = heatmap_df.values\n",
    "\n",
    "    # Normalize the 'Difference' values for color mapping\n",
    "    if vmin is None:\n",
    "        vmin = filtered_df[\"Difference\"].min()\n",
    "\n",
    "    if vmax is None:\n",
    "        vmax = filtered_df[\"Difference\"].max()\n",
    "    midpoint = 0  # Midpoint for color gradient\n",
    "\n",
    "    # Print the vmin and vmax values - for debuGGINH\n",
    "    # print(f\"vmin: {vmin}, vmax: {vmax}\")\n",
    "\n",
    "    rows = []\n",
    "    for idx, values in enumerate(heatmap_values):\n",
    "        model_name = custom_model_order[idx]\n",
    "        formatted_values = [\n",
    "            f\"\\\\midpointgradientcell{{{value:.1f}}}{{{vmin}}}{{{vmax}}}{{0}}{{{neg_color_code}}}{{{pos_color_code}}}{{\\\\opacity}}{{0}}\"\n",
    "            for value in values\n",
    "        ]\n",
    "        # Create a new row using the template\n",
    "        new_row = f\"&\\\\texttt{{{model_name_mapping[model_name]}}} & \" + \" & \".join(formatted_values) + \" \\\\\\\\\"\n",
    "        rows.append(new_row)\n",
    "\n",
    "    # Join all rows into a single string and print it\n",
    "    table = \"\\n\".join(rows)\n",
    "    return table, vmin, vmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_reranking_table, before_reranking_vmin, before_reranking_vmax = generate_latex(check_that_before_ranking_results_df, exclude_experiments=['Amplify/Downplay'])\n",
    "print(before_reranking_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recovery_table, recovery_vmin, recovery_vmax = generate_latex(\n",
    "    recovery_check_that_df, \n",
    "    exclude_experiments=['Amplify/Downplay'],\n",
    "    pos_color_code=\"rerank_pos\",\n",
    "    vmin=None,\n",
    "    vmax=None)\n",
    "print(recovery_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_pipe_table, overall_vmin, overall_vmax = generate_latex(\n",
    "    e2e_results_check_that_df, \n",
    "    vmin=before_reranking_vmin,\n",
    "    vmax=before_reranking_vmax,\n",
    "    exclude_experiments=['Amplify/Downplay'])\n",
    "print(overall_pipe_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import cmcrameri.cm as cmc\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def parse_map_differences(file_path: str):\n",
    "    \"\"\"\n",
    "    Parses a single JSON file to calculate MAP@20 differences.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            results.append(json.loads(line))\n",
    "    \n",
    "    data = {}\n",
    "    for entry in results:\n",
    "        dialect = entry['dialect']\n",
    "        for model, values in entry.items():\n",
    "            if model == \"dialect\":\n",
    "                continue\n",
    "            \n",
    "            original_baseline = values[\"original_baseline\"][\"map_results\"][\"test\"][-1]\n",
    "            edited_baseline = values[\"edited_baseline\"][\"map_results\"][\"test\"][-1]\n",
    "            difference = (edited_baseline - original_baseline) * 100  # Scale to %\n",
    "            \n",
    "            if model not in data:\n",
    "                data[model] = {}\n",
    "            data[model][dialect] = round(difference, 2)\n",
    "    \n",
    "    return pd.DataFrame(data).T\n",
    "\n",
    "def plot_map_differences(json_files_dict, output_path:str, model_order=None):\n",
    "    \"\"\"\n",
    "    Plots a heatmap of MAP differences for multiple experiments and dialects.\n",
    "\n",
    "    Args:\n",
    "        json_files_dict (dict): Mapping of experiment names to JSON file paths.\n",
    "        model_order (list): Optional ordering of model names.\n",
    "    \"\"\"\n",
    "    # Set global font\n",
    "    matplotlib.rcParams['font.family'] = 'Helvetica Neue'\n",
    "\n",
    "    # Parse data from all experiments\n",
    "    all_results = []\n",
    "    for experiment_name, file_path in json_files_dict.items():\n",
    "        df = parse_map_differences(file_path)\n",
    "        df = df.rename_axis(\"Model\").reset_index()\n",
    "        df[\"Experiment\"] = experiment_name\n",
    "        all_results.append(df)\n",
    "\n",
    "    # Combine all experiment data into a single DataFrame\n",
    "    combined_df = pd.concat(all_results, ignore_index=True)\n",
    "    combined_df = combined_df.melt(id_vars=[\"Model\", \"Experiment\"], var_name=\"Dialect\", value_name=\"MAP_Difference\")\n",
    "\n",
    "    # Pivot the data for heatmap\n",
    "    heatmap_data = combined_df.pivot_table(index=\"Model\", columns=[\"Dialect\"], values=\"MAP_Difference\")\n",
    "\n",
    "    # Reorder models if specified\n",
    "    if model_order:\n",
    "        heatmap_data = heatmap_data.reindex(model_order)\n",
    "\n",
    "    # Annotate with formatted values\n",
    "    annot_data = heatmap_data.applymap(lambda x: f\"{x:.1f}%\" if not pd.isna(x) else \"\")\n",
    "\n",
    "    # Plot the heatmap\n",
    "    cmap = cmc.vik_r  # Diverging colormap\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    ax = sns.heatmap(\n",
    "        heatmap_data,\n",
    "        cmap=cmap,\n",
    "        center=0,\n",
    "        annot=annot_data,\n",
    "        fmt=\"\",\n",
    "        linewidths=0.5,\n",
    "        cbar_kws={\"orientation\": \"horizontal\", \"shrink\": 0.8}\n",
    "    )\n",
    "\n",
    "    # Add title and format\n",
    "    ax.set_ylabel(\"Models\")\n",
    "    ax.xaxis.tick_top()  # Move column labels to the top\n",
    "    ax.xaxis.set_label_position(\"top\")\n",
    "    ax.set_xlabel(\"\")  # Remove xlabel\n",
    "\n",
    "\n",
    "    # Add a border around the heatmap\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(True)\n",
    "        spine.set_linewidth(2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches=\"tight\")  # Save with high DPI\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = \"../experiments/dialect/gpt4o/clef2021-checkthat-task2a--english/results/before_reranking_dialect_results.jsonl\"\n",
    "# json_files = {\n",
    "#     'experiment': file_path\n",
    "\n",
    "# }\n",
    "# output_path= 'dialect_map_differences.png'\n",
    "# plot_map_differences(json_files, output_path, custom_model_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change setting to allow dataframe to be printed in full witdh\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_named_entity_replacement = pd.read_csv(\"/home/kebl7383/claim-matching-robustness/experiments/named_entity_replacement/gpt4o/cl/orig_worstcase_named_entity_replacements.tsv\", sep=\"\\t\") \n",
    "edited_named_entity_replacement = pd.read_csv(\"/home/kebl7383/claim-matching-robustness/experiments/named_entity_replacement/gpt4o/fact-check-tweet/edited_worstcase_named_entity_replacements.tsv\", sep=\"\\t\")  \n",
    "# Merge the two dataframes\n",
    "merged_df = pd.merge(baseline_named_entity_replacement, edited_named_entity_replacement, on=\"query_id\", suffixes=('_baseline', '_edited'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import jsonlines\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Define the directory containing JSONL files\n",
    "# data_dir = \"../experiments/named_entity_replacement/gpt4o/clef2021-checkthat-task2a--english/results\"  # Update this to your directory path\n",
    "# n_candidates_list = [5, 10, 20, 50, 100]\n",
    "\n",
    "# # Store absolute differences for plotting\n",
    "# baseline_differences = {n: [] for n in n_candidates_list}\n",
    "# worstcase_differences = {n: [] for n in n_candidates_list}\n",
    "\n",
    "# for n_candidates in n_candidates_list:\n",
    "#     file_path = os.path.join(data_dir, f\"after_reranking_n_candidates_{n_candidates}_results.jsonl\")\n",
    "    \n",
    "#     if not os.path.exists(file_path):\n",
    "#         print(f\"File {file_path} not found. Skipping...\")\n",
    "#         continue\n",
    "\n",
    "#     # Open and read the JSONL file\n",
    "#     with jsonlines.open(file_path) as reader:\n",
    "#         for obj in reader:\n",
    "#             for model, data in obj.items():\n",
    "#                 # Extract map@5 values\n",
    "#                 original_baseline_map5 = data[\"original_baseline\"][\"map_results\"][\"test\"][1]\n",
    "#                 edited_baseline_map5 = data[\"edited_baseline\"][\"map_results\"][\"test\"][1]\n",
    "#                 original_worstcase_map5 = data[\"original_worstcase\"][\"map_results\"][\"test\"][1]\n",
    "#                 edited_worstcase_map5 = data[\"edited_worstcase\"][\"map_results\"][\"test\"][1]\n",
    "\n",
    "#                 # Calculate absolute differences\n",
    "#                 baseline_diff = original_baseline_map5 - edited_baseline_map5\n",
    "#                 worstcase_diff = original_worstcase_map5 - edited_worstcase_map5\n",
    "                \n",
    "#                 # Append to lists\n",
    "#                 baseline_differences[n_candidates].append(baseline_diff)\n",
    "#                 worstcase_differences[n_candidates].append(worstcase_diff)\n",
    "\n",
    "# # Prepare data for plotting\n",
    "# baseline_plot_data = [sum(baseline_differences[n]) for n in n_candidates_list]\n",
    "# worstcase_plot_data = [sum(worstcase_differences[n]) for n in n_candidates_list]\n",
    "\n",
    "# # Step 2: Plot the Results\n",
    "# plt.figure(figsize=(8, 5))\n",
    "\n",
    "# # Plot baseline and worstcase\n",
    "# plt.plot(\n",
    "#     n_candidates_list, \n",
    "#     baseline_plot_data, \n",
    "#     marker='o', \n",
    "#     markersize=8,  # Thicker markers\n",
    "#     label=\"Baseline (MAP@5)\", \n",
    "#     linestyle='-'\n",
    "# )\n",
    "# plt.plot(\n",
    "#     n_candidates_list, \n",
    "#     worstcase_plot_data, \n",
    "#     marker='x', \n",
    "#     markersize=8,  # Thicker markers\n",
    "#     label=\"Worstcase (MAP@5)\", \n",
    "#     linestyle='--'\n",
    "# )\n",
    "\n",
    "# # Add a horizontal line at y=0\n",
    "# plt.axhline(0, color='gray', linestyle=':', linewidth=1)\n",
    "\n",
    "# # Add grid with dotted lines\n",
    "# plt.grid(linestyle=':')\n",
    "\n",
    "# # Configure the plot\n",
    "# plt.title(\"Absolute Difference in MAP@5 for Different n_candidates\")\n",
    "# plt.xlabel(\"n_candidates\")\n",
    "# plt.ylabel(\"Absolute Difference in MAP@5\")\n",
    "# plt.legend()\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import jsonlines\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cm import get_cmap\n",
    "import cmcrameri.cm as cmc\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def plot_comparison(data_dir: str, n_candidates_list: list, transformation_name: str):\n",
    "\n",
    "    # Store MAP@5 values for plotting\n",
    "    original_baseline_map5_values = {n: [] for n in n_candidates_list}\n",
    "    edited_baseline_map5_values = {n: [] for n in n_candidates_list}\n",
    "    original_worstcase_map5_values = {n: [] for n in n_candidates_list}\n",
    "    edited_worstcase_map5_values = {n: [] for n in n_candidates_list}\n",
    "\n",
    "    for n_candidates in n_candidates_list:\n",
    "        file_path = os.path.join(data_dir, f\"after_reranking_n_candidates_{n_candidates}_results.jsonl\")\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"File {file_path} not found. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Open and read the JSONL file\n",
    "        with jsonlines.open(file_path) as reader:\n",
    "            for obj in reader:\n",
    "                for model, data in obj.items():\n",
    "                    # Extract map@5 values\n",
    "                    original_baseline_map5 = data[\"original_baseline\"][\"map_results\"][\"test\"][1] * 100  # Convert to percentage\n",
    "                    edited_baseline_map5 = data[\"edited_baseline\"][\"map_results\"][\"test\"][1] * 100  # Convert to percentage\n",
    "                    original_worstcase_map5 = data[\"original_worstcase\"][\"map_results\"][\"test\"][1] * 100  # Convert to percentage\n",
    "                    edited_worstcase_map5 = data[\"edited_worstcase\"][\"map_results\"][\"test\"][1] * 100  # Convert to percentage\n",
    "\n",
    "                    # Append MAP@5 values\n",
    "                    original_baseline_map5_values[n_candidates].append(original_baseline_map5)\n",
    "                    edited_baseline_map5_values[n_candidates].append(edited_baseline_map5)\n",
    "                    original_worstcase_map5_values[n_candidates].append(original_worstcase_map5)\n",
    "                    edited_worstcase_map5_values[n_candidates].append(edited_worstcase_map5)\n",
    "\n",
    "    # Combined plot for baseline and worstcase\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "    # Create equal spacing for x-axis labels\n",
    "    x_positions = range(len(n_candidates_list))\n",
    "\n",
    "    # Colormap for visual appeal\n",
    "    cmap = cmc.navia_r\n",
    "    baseline_color = cmap(0.3)  # Light color for baseline\n",
    "    worstcase_color = cmap(0.6)  # Darker color for worstcase\n",
    "\n",
    "    # Define unique markers for n_candidates\n",
    "    markers = ['o', 's', '^', 'D', 'P', '*', 'h']  # Circle, square, triangle, diamond, plus\n",
    "\n",
    "    # Plot for each n_candidates\n",
    "    for i, n_candidates in enumerate(n_candidates_list):\n",
    "        # Offset positions for baseline and worstcase\n",
    "        baseline_position = x_positions[i] - 0.15  # Slightly left\n",
    "        worstcase_position = x_positions[i] + 0.15  # Slightly right\n",
    "\n",
    "        # Baseline original markers\n",
    "        ax.scatter([baseline_position] * len(original_baseline_map5_values[n_candidates]),\n",
    "                original_baseline_map5_values[n_candidates],\n",
    "                label=\"Baseline\" if i == 0 else \"\",  # Add label only once\n",
    "                marker=markers[i],\n",
    "                color=baseline_color, edgecolor=\"black\", alpha=0.8)\n",
    "        \n",
    "        # Baseline box plot\n",
    "        all_baseline_values = original_baseline_map5_values[n_candidates] + edited_baseline_map5_values[n_candidates]\n",
    "        box = ax.boxplot(\n",
    "            all_baseline_values,\n",
    "            positions=[baseline_position],\n",
    "            widths=0.2,\n",
    "            vert=True,\n",
    "            showmeans=False,\n",
    "            patch_artist=True  # Fill the box with color\n",
    "        )\n",
    "        for patch in box['boxes']:\n",
    "            patch.set_facecolor(baseline_color)\n",
    "            patch.set_alpha(0.7)\n",
    "        for median in box.get('medians', []):\n",
    "            median.set_visible(False)\n",
    "\n",
    "        # Worstcase original markers\n",
    "        ax.scatter([worstcase_position] * len(original_worstcase_map5_values[n_candidates]),\n",
    "                original_worstcase_map5_values[n_candidates],\n",
    "                label=\"Worstcase\" if i == 0 else \"\",  # Add label only once\n",
    "                marker=markers[i],\n",
    "                color=worstcase_color, edgecolor=\"black\", alpha=0.8)\n",
    "        \n",
    "        # Worstcase box plot\n",
    "        all_worstcase_values = original_worstcase_map5_values[n_candidates] + edited_worstcase_map5_values[n_candidates]\n",
    "        box = ax.boxplot(\n",
    "            all_worstcase_values,\n",
    "            positions=[worstcase_position],\n",
    "            widths=0.2,\n",
    "            vert=True,\n",
    "            showmeans=False,\n",
    "            patch_artist=True  # Fill the box with color\n",
    "        )\n",
    "        for patch in box['boxes']:\n",
    "            patch.set_facecolor(worstcase_color)\n",
    "            patch.set_alpha(0.7)\n",
    "        for median in box.get('medians', []):\n",
    "            median.set_visible(False)\n",
    "\n",
    "    # Configure plot\n",
    "    ax.set_title(f\"Baseline and Worstcase MAP@5 Across Candidates for {transformation_name}\", fontsize=12)\n",
    "    ax.set_xlabel(\"Number of Candidates\", fontsize=10)\n",
    "    ax.set_ylabel(\"MAP@5 (%)\", fontsize=10)\n",
    "    ax.set_xticks(x_positions)\n",
    "    ax.set_xticklabels(n_candidates_list, fontsize=10)\n",
    "    ax.grid(linestyle=':', alpha=0.6)\n",
    "    ax.legend(fontsize=10, frameon=False)\n",
    "\n",
    "    # Final adjustments\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory containing JSONL files\n",
    "data_dir = \"../experiments/named_entity_replacement/gpt4o/clef2021-checkthat-task2a--english/results\" \n",
    "n_candidates_list = [5, 10, 20, 50, 100, 200]\n",
    "plot_comparison(data_dir, n_candidates_list, transformation_name=\"Named Entity Replacement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory containing JSONL files\n",
    "data_dir = \"../experiments/typos/gpt4o/clef2021-checkthat-task2a--english/results\" \n",
    "n_candidates_list = [5, 10, 20, 50, 100, 200]\n",
    "plot_comparison(data_dir, n_candidates_list, transformation_name=\"Typos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory containing JSONL files\n",
    "data_dir = \"../experiments/rewrite/gpt4o/clef2021-checkthat-task2a--english/results\" \n",
    "n_candidates_list = [5, 10, 20, 50, 100, 200]\n",
    "plot_comparison(data_dir, n_candidates_list, transformation_name=\"Rewrites\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import jsonlines\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_baseline_performance(data_dir: str, n_candidates_list: list, transformation_name: str, output_file):\n",
    "    \"\"\"\n",
    "    Plots a bar chart showing the original baseline MAP@5 performance across different numbers of candidates,\n",
    "    treating the x-axis as categorical to avoid scaling differences.\n",
    "    \n",
    "    Parameters:\n",
    "        data_dir (str): Directory containing the JSONL result files.\n",
    "        n_candidates_list (list): List of candidate numbers (top-k values).\n",
    "        transformation_name (str): Name of the transformation (used in the title).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Store MAP@5 values for the baseline model\n",
    "    baseline_map5_values = []\n",
    "\n",
    "    for n_candidates in n_candidates_list:\n",
    "        file_path = os.path.join(data_dir, f\"after_reranking_n_{n_candidates}_appendix.jsonl\")\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"File {file_path} not found. Skipping...\")\n",
    "            baseline_map5_values.append(None)  # Append None for missing data\n",
    "            continue\n",
    "\n",
    "        # Read the JSONL file\n",
    "        map5_scores = []\n",
    "        with jsonlines.open(file_path) as reader:\n",
    "            for obj in reader:\n",
    "                for model, data in obj.items():\n",
    "                    # Extract original baseline MAP@5 (converted to percentage)\n",
    "                    map5_scores.append(data[\"original_baseline\"][\"map_results\"][\"test\"][1] * 100)\n",
    "\n",
    "        # Store the mean MAP@5 for the given number of candidates\n",
    "        if map5_scores:\n",
    "            baseline_map5_values.append(np.mean(map5_scores))\n",
    "        else:\n",
    "            baseline_map5_values.append(None)\n",
    "\n",
    "    # Filter out missing data\n",
    "    valid_indices = [i for i, v in enumerate(baseline_map5_values) if v is not None]\n",
    "    valid_n_candidates = [n_candidates_list[i] for i in valid_indices]\n",
    "    valid_map5_values = [baseline_map5_values[i] for i in valid_indices]\n",
    "\n",
    "    # Treat the number of candidates as categorical\n",
    "    x_positions = np.arange(len(valid_n_candidates))  # Create categorical x-axis positions\n",
    "\n",
    "    # Plot settings\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    cmap = cmc.navia\n",
    "    baseline_color = cmap(0.3) \n",
    "    bar_width = 0.7  # Increased bar width\n",
    "    plt.bar(x_positions, valid_map5_values, color=baseline_color, alpha=0.8, edgecolor=\"black\", width=bar_width)\n",
    "\n",
    "    # Identify the top performer\n",
    "    max_value = max(valid_map5_values)\n",
    "    max_index = valid_map5_values.index(max_value)\n",
    "\n",
    "    # Add text labels on top of bars (with an asterisk for the top performer)\n",
    "    for i, v in enumerate(valid_map5_values):\n",
    "        label = f\"{v:.1f}\" + (\" *\" if i == max_index else \"\")\n",
    "        plt.text(x_positions[i], v + 0.5, label, ha=\"center\", fontsize=16, fontweight=\"bold\" if i == max_index else \"normal\")\n",
    "\n",
    "    # Configure categorical x-axis\n",
    "    plt.xlabel(\"Number of Candidates (top-j)\", fontsize=20)\n",
    "    plt.ylabel(\"MAP@5 (%)\", fontsize=20)\n",
    "    # plt.title(f\"Baseline MAP@5 Performance Across Candidates ({transformation_name})\", fontsize=14)\n",
    "\n",
    "    plt.xticks(x_positions, valid_n_candidates, fontsize=18)  # Treat as categorical\n",
    "    plt.yticks(fontsize=20)\n",
    "\n",
    "    # Set y-axis to start from 50\n",
    "    plt.ylim(70, max(valid_map5_values) + 5)\n",
    "\n",
    "    # Remove y-grid for a cleaner look\n",
    "    plt.grid(axis=\"y\", linestyle=\":\", alpha=0.6)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file, format='pdf', bbox_inches='tight', dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory containing JSONL files\n",
    "data_dir = \"../experiments/rewrite/gpt4o/clef2021-checkthat-task2a--english/results\" \n",
    "n_candidates_list = [5, 10, 20, 50, 100, 200]\n",
    "plot_baseline_performance(data_dir, n_candidates_list, transformation_name=\"Typos\", output_file=\"baseline_performance_bge.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.stats.inter_rater import fleiss_kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('../annotations/annotator1/dialect_for_annotation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define the main annotations folder\n",
    "annotations_folder = \"../annotations\"\n",
    "\n",
    "# Define perturbation types\n",
    "perturbation_types = [\n",
    "    \"dialect_for_annotation.csv\",\n",
    "    \"entity_for_annotation.csv\",\n",
    "    \"typos_for_annotation.csv\"\n",
    "]\n",
    "\n",
    "# Expected number of queries per file\n",
    "expected_count = 20\n",
    "\n",
    "# Initialize a dictionary to store results\n",
    "results = {}\n",
    "\n",
    "for perturbation in perturbation_types:\n",
    "    dataframes = {}\n",
    "    \n",
    "    for annotator in [\"annotator1\", \"annotator2\", \"annotator3\"]:\n",
    "        file_path = os.path.join(annotations_folder, annotator, perturbation)\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Check completeness by verifying the number of rows\n",
    "        if len(df) != expected_count:\n",
    "            print(f\"WARNING: {annotator}'s file for {perturbation} is incomplete: found {len(df)} rows (expected {expected_count}).\")\n",
    "        dataframes[annotator] = df.reset_index(drop=True)\n",
    "    \n",
    "    # Merge on index by creating a new DataFrame for perturbation_accuracy\n",
    "    perturbation_accuracy_df = pd.DataFrame({\n",
    "        \"annotator1\": dataframes[\"annotator1\"][\"perturbation_accuracy\"],\n",
    "        \"annotator2\": dataframes[\"annotator2\"][\"perturbation_accuracy\"],\n",
    "        \"annotator3\": dataframes[\"annotator3\"][\"perturbation_accuracy\"]\n",
    "    })\n",
    "    \n",
    "    # Compute majority perturbation accuracy:\n",
    "    # A query is considered valid only if all three annotators mark it as valid (i.e., value 1).\n",
    "    perturbation_accuracy_df[\"majority_perturbation_accuracy\"] = (\n",
    "        perturbation_accuracy_df.sum(axis=1) == 3\n",
    "    ).astype(int)\n",
    "    \n",
    "    # Print queries (by index) where the majority perturbation accuracy is not 1\n",
    "    non_valid_queries = perturbation_accuracy_df[perturbation_accuracy_df[\"majority_perturbation_accuracy\"] != 1]\n",
    "    if not non_valid_queries.empty:\n",
    "        print(f\"Perturbation: {perturbation}\")\n",
    "        print(\"Queries where majority perturbation accuracy is not 1:\")\n",
    "        print(non_valid_queries)\n",
    "        print()\n",
    "    \n",
    "    # Store the results\n",
    "    results[perturbation] = {\n",
    "        \"total_queries\": len(perturbation_accuracy_df),\n",
    "        \"average_perturbation_accuracy\": perturbation_accuracy_df[\"majority_perturbation_accuracy\"].mean()\n",
    "    }\n",
    "\n",
    "# (Optional) Print the final aggregated results\n",
    "for perturbation, metrics in results.items():\n",
    "    print(f\"Perturbation: {perturbation}\")\n",
    "    print(f\"Total Queries: {metrics['total_queries']}\")\n",
    "    print(f\"Average Perturbation Accuracy (Majority): {metrics['average_perturbation_accuracy']:.3f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
