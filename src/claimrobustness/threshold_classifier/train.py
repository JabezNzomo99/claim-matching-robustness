# Python script to train a simple threshold classifier based on the cosine similarities between the claim and all the fact checks
# Takes in as input the train file used to train the verifier
# Input contains positive mined instances - claim, fact check pairs that actually matched
# Negative instances are generated by pairing the claim with the second closest fact check i.e. hard negatives
# For each embedding model, load the model
# Encode the claims and the fact checks
# Compute the cosine similarity between the claim and the fact checks
# Define a list of thresholds for k-fold cross validation
# For each threshold, perform 10-fold cross validation
# Select the threshold that maximizes the F1 score and save it on a json file - compute the F1 score as well
import os
import argparse
import configparser
from pathlib import Path
from claimrobustness import utils
import jsonlines
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
import numpy as np
import torch
from sklearn.metrics import f1_score
from sklearn.model_selection import KFold


# Function to compute cosine similarity element-wise
def cosine_similarity(a, b):
    a_norm = a / np.linalg.norm(a, axis=1, keepdims=True)
    b_norm = b / np.linalg.norm(b, axis=1, keepdims=True)
    cosine_similarities = np.sum(a_norm * b_norm, axis=1)
    return cosine_similarities


def cosine_similarity_llm(a, b):
    # Normalize the embeddings
    a_norm = a / a.norm(dim=1, keepdim=True)
    b_norm = b / b.norm(dim=1, keepdim=True)
    # Compute cosine similarity using matrix multiplication
    cosine_similarities = torch.sum(a_norm * b_norm, dim=1)
    return cosine_similarities.cpu().numpy()


def run():
    parser = argparse.ArgumentParser(description="Train a simple threshold classifier")
    parser.add_argument(
        "experiment_path",
        help="Path where config lies",
        type=str,
    )
    args = parser.parse_args()
    config = configparser.ConfigParser()
    config.read(os.path.join(args.experiment_path, "config.ini"))
    dataset_dir = config["data"].get("dataset_dir")

    # Check whether train verifier dataset exists
    if (
        Path(dataset_dir).is_dir() == False
        or (Path.cwd() / dataset_dir / "train_verifier_dataset.csv").exists() == False
    ):
        raise ValueError(
            f"Dataset {dataset_dir} not found, run create-verifer-dataset first"
        )

    # Load the dataset
    train_data = utils.load_verifier_data(
        dataset_path=os.path.join(dataset_dir, "train_verifier_dataset.csv")
    )
    dev_data = utils.load_verifier_data(
        dataset_path=os.path.join(dataset_dir, "dev_verifier_dataset.csv")
    )
    test_data = utils.load_verifier_data(
        dataset_path=os.path.join(dataset_dir, "test_verifier_dataset.csv")
    )

    print("Train data shape: ", train_data.shape)
    print("Dev data shape: ", dev_data.shape)
    print("Test data shape: ", test_data.shape)

    embedding_models = config["evaluation"].get("embedding_models").split(",")
    print(f"Running evaluation on the following models: {embedding_models}")

    # Create a results directory if not exists
    save_dir = f"{args.experiment_path}/results"
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)

    # Before running the experiments, check if results instance already exists for model
    device = utils.get_device()
    results_file_path = os.path.join(
        save_dir, "threshold_classifier_results_finegrained.jsonl"
    )
    # Load existing results to check for already evaluated models
    evaluated_models = set()
    if os.path.exists(results_file_path):
        with jsonlines.open(results_file_path, mode="r") as reader:
            for obj in reader:
                for model in embedding_models:
                    if model in obj:
                        evaluated_models.add(model)

    with jsonlines.open(results_file_path, mode="a") as writer:
        for embedding_model_path in tqdm(embedding_models):
            # Check if the model has already been evaluated
            if embedding_model_path in evaluated_models:
                print(f"Skipping evaluation for model: {embedding_model_path}")
                continue

            print(f"Running evaluation on model: {embedding_model_path}")

            if embedding_model_path == "LLM2Vec-Meta-Llama-3-8B":
                llm_encoder = utils.init_llm_encoder()
                train_query_embs = utils.encode_with_llm(
                    model=llm_encoder,
                    sentences=train_data["query"].to_list(),
                    encoding_type=utils.EncodingType.QUERY,
                    instruction="Given a claim, retrieve relevant fact checks that match the claim:",
                )
                train_claim_embs = utils.encode_with_llm(
                    model=llm_encoder,
                    sentences=train_data["claim"].to_list(),
                    encoding_type=utils.EncodingType.DOCUMENT,
                )
                test_query_embs = utils.encode_with_llm(
                    model=llm_encoder,
                    sentences=test_data["query"].to_list(),
                    encoding_type=utils.EncodingType.QUERY,
                    instruction="Given a claim, retrieve relevant fact checks that match the claim:",
                )
                test_claim_embs = utils.encode_with_llm(
                    model=llm_encoder,
                    sentences=test_data["claim"].to_list(),
                    encoding_type=utils.EncodingType.DOCUMENT,
                )
            else:
                model = SentenceTransformer(embedding_model_path)
                train_query_embs = model.encode(
                    train_data["query"].to_list(),
                    prompt="Represent the fact for retrieving supporting evidence:",
                    show_progress_bar=True,
                    device=device,
                )
                train_claim_embs = model.encode(
                    train_data["claim"].to_list(),
                    prompt="Represent the evidence for retrieval:",
                    show_progress_bar=True,
                    device=device,
                )
                test_query_embs = model.encode(
                    test_data["query"].to_list(),
                    prompt="Represent the fact for retrieving supporting evidence:",
                    show_progress_bar=True,
                    device=device,
                )
                test_claim_embs = model.encode(
                    test_data["claim"].to_list(),
                    prompt="Represent the evidence for retrieval:",
                    show_progress_bar=True,
                    device=device,
                )

            if embedding_model_path == "LLM2Vec-Meta-Llama-3-8B":
                train_cosine_similarities = cosine_similarity_llm(
                    train_query_embs, train_claim_embs
                ).flatten()
                test_cosine_similarities = cosine_similarity_llm(
                    test_query_embs, test_claim_embs
                ).flatten()
            else:
                train_cosine_similarities = cosine_similarity(
                    train_query_embs, train_claim_embs
                ).flatten()
                test_cosine_similarities = cosine_similarity(
                    test_query_embs, test_claim_embs
                ).flatten()

            train_labels = train_data["label"].to_numpy().flatten()
            true_test_labels = test_data["label"].to_numpy().flatten()

            print(train_cosine_similarities.shape)
            print(test_cosine_similarities.shape)

            # Ensure that labels are binary
            train_labels = train_labels.astype(int)
            true_test_labels = true_test_labels.astype(int)

            print(train_cosine_similarities.shape)
            print(test_cosine_similarities.shape)
            print(train_labels.shape)

            # Check the lengths
            assert len(train_cosine_similarities) == len(
                train_labels
            ), "Mismatch in lengths of train similarities and labels"
            assert len(test_cosine_similarities) == len(
                true_test_labels
            ), "Mismatch in lengths of test similarities and labels"

            # Define a comprehensive range of thresholds to evaluate
            thresholds = np.arange(0.5, 1.01, 0.01)

            # Initialize variables to store the best threshold and best F1 score
            best_threshold = 0.0
            best_f1 = 0.0

            # Number of folds for cross-validation
            num_folds = 10
            kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)

            for threshold in thresholds:
                fold_f1_scores = []
                # Perform 10-fold cross-validation
                for train_index, val_index in kf.split(train_cosine_similarities):
                    # Split train similarities and labels into training and validation sets
                    _, fold_val_similarities = (
                        train_cosine_similarities[train_index],
                        train_cosine_similarities[val_index],
                    )
                    _, fold_val_labels = (
                        train_labels[train_index],
                        train_labels[val_index],
                    )

                    # Predict matches based on the current threshold
                    predicted_val_labels = (fold_val_similarities >= threshold).astype(
                        int
                    )

                    # Ensure the labels are binary
                    fold_val_labels_binary = fold_val_labels.astype(int)

                    # Compute the F1 score for the current fold
                    f1 = f1_score(fold_val_labels_binary, predicted_val_labels)
                    fold_f1_scores.append(f1)

                # Calculate the average F1 score across all folds for the current threshold
                avg_f1 = np.mean(fold_f1_scores)

                # Update the best threshold if the current average F1 score is higher
                if avg_f1 > best_f1:
                    best_f1 = avg_f1
                    best_threshold = threshold

            # Evaluate best threshold on test set
            predicted_test_labels = (test_cosine_similarities >= best_threshold).astype(
                int
            )
            test_f1 = f1_score(true_test_labels, predicted_test_labels)

            results = {
                "test_f1_score": test_f1,
                "train_f1_score": best_f1,
                "best_threshold": best_threshold,
            }

            embedding_model_results = {embedding_model_path: results}
            writer.write(embedding_model_results)


if __name__ == "__main__":
    run()
