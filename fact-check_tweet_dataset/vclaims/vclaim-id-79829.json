{"title": "V\u00eddeos falsos feitos com intelig\u00eancia artificial podem ser danosos em campanhas eleitorais", "subtitle": "Tecnologia para produzir os chamados 'deepfakes' evoluiu muito nos \u00faltimos anos; ainda n\u00e3o existem m\u00e9todos para", "author": "politica.estadao.com.br", "date": "2018-06-25T09:37:17-03:00", "vclaim_id": "vclaim-id-79829", "url": "https://politica.estadao.com.br/blogs/estadao-verifica/videos-falsos-feitos-com-inteligencia-artificial-podem-ser-danosos-em-campanhas-eleitorais/", "vclaim": "Pense na situa\u00e7\u00e3o: o ano \u00e9 2020 e a situa\u00e7\u00e3o diplom\u00e1tica entre Estados Unidos e Coreia do Norte \u00e9 tensa. Uma rede de televis\u00e3o recebe uma filmagem in\u00e9dita, de fonte an\u00f4nima, que mostra o l\u00edder Kim Jong-un discutindo com generais o lan\u00e7amento de um ataque nuclear. Na Casa Branca, o v\u00eddeo \u00e9 analisado, mas a intelig\u00eancia n\u00e3o consegue verificar a autenticidade. O presidente americano tem que agir \u2014 e ordena um contra-ataque. Uma guerra come\u00e7a.\n\n+++Recebeu algum boato pelo WhatsApp? Envie para o Estad\u00e3o Verifica\n\nO pesquisador Giorgio Patrini, da Universidade de Amsterd\u00e3, na Holanda, imaginou o cen\u00e1rio acima para alertar sobre uma preocupante modalidade de informa\u00e7\u00e3o falsa: os . S\u00e3o v\u00eddeos que simulam cenas aplicando t\u00e9cnicas de intelig\u00eancia artificial a imagens existentes. At\u00e9 pouco tempo, o fen\u00f4meno se restringia ao mundo de filmes adultos. Mas, com o avan\u00e7o da tecnologia de\n\n\u201cSe houver incentivo financeiro ou pol\u00edtico suficiente para que atores mal-intencionados fa\u00e7am isso, \u00e9 inteiramente poss\u00edvel que a fabrica\u00e7\u00e3o de v\u00eddeos seja usada em futuras campanhas pol\u00edticas\u201d, diz Patrini.\n\nPatrini ressalta que, de t\u00e3o forte o impacto de um v\u00eddeo falso, a mensagem manipulada tende a ser aceita por quem quer acreditar nela \u2014 mesmo que comprovada a falsidade. \u201cOs psic\u00f3logos nos advertem de duas fal\u00e1cias humanas: a tend\u00eancia a acreditar em informa\u00e7\u00f5es falsas ap\u00f3s exposi\u00e7\u00e3o repetida (efeito de verdade ilus\u00f3rio) e a acreditar em informa\u00e7\u00f5es falsas quando apenas confirma nossas cren\u00e7as anteriores, ou seja, vi\u00e9s de confirma\u00e7\u00e3o.\u201d\n\nO cientista da computa\u00e7\u00e3o brasileiro Virgilio Almeida, professor associado de Harvard, alerta, no entanto, que a produ\u00e7\u00e3o de v\u00eddeos falsos sem grande refinamento j\u00e1 \u00e9 acess\u00edvel hoje. \u201cSe pensarmos que j\u00e1 em 2014 usaram bots, isso certamente vai ser utilizado. Mas as pessoas acreditam muito no que veem. Muitos n\u00e3o v\u00e3o acreditar, mas muitos v\u00e3o. O processo eleitoral passa a ser muito levado por isso.\u201d\n\nSe as elei\u00e7\u00f5es deste ano j\u00e1 podem ter o rebuli\u00e7o do compartilhamento de v\u00eddeos falsos, Patrini salienta que as poss\u00edveis repercuss\u00f5es da manipula\u00e7\u00e3o audiovisual v\u00e3o al\u00e9m da pol\u00edtica. Considere, por exemplo, o uso de \u00e1udios e v\u00eddeos como provas em processos criminais. Se n\u00e3o pudermos mais confiar em sua autenticidade, como poderemos aceit\u00e1-los como evid\u00eancia?\n\nSolu\u00e7\u00f5es. Se o progn\u00f3stico das deepfakes \u00e9 pessimista, isso se d\u00e1, em grande parte, porque ainda n\u00e3o h\u00e1 t\u00e9cnica desenvolvida para identificar os audiovisuais falsos. \u201c\u00c9 muito dif\u00edcil descobrir v\u00eddeos falsos e n\u00e3o existe detectores em larga escala. As t\u00e9cnicas de per\u00edcia digital est\u00e3o muito atr\u00e1s\u201d, aponta Almeida.\n\nPatrini sugere que a tecnologia de defesa contra deepfakes siga dois caminhos. O primeiro seria a cria\u00e7\u00e3o de uma assinatura digital em v\u00eddeos \u2013 an\u00e1loga a marcas d\u2019\u00e1gua em notas de dinheiro \u2013 que garantiria a autenticidade da c\u00e2mera que deu origem a um filme e a aus\u00eancia de edi\u00e7\u00f5es. No entanto, uma assinatura digital seria invalidada por qualquer tipo de edi\u00e7\u00e3o, mesmo os \u2018benignos\u2019 \u2013 incluindo mudan\u00e7a de contraste e pequenos cortes. Al\u00e9m disso, n\u00e3o haveria como assegurar a veracidade de v\u00eddeos antigos, anteriores a uma poss\u00edvel implementa\u00e7\u00e3o de assinaturas.\n\nOutra solu\u00e7\u00e3o vai na linha do \u2018feiti\u00e7o contra o feiticeiro\u2019: usar intelig\u00eancia artificial e machine learning para criar detectores de v\u00eddeos falsos. A ideia \u00e9 treinar computadores para identificar sinais de adultera\u00e7\u00e3o que seriam invis\u00edveis a olhos humanos. O pesquisador considera esta a melhor op\u00e7\u00e3o.\n\n\u201cAssim como o aprendizado de m\u00e1quina nos possibilita meios poderosos de manipula\u00e7\u00e3o da m\u00eddia, ele pode resgatar e trabalhar como um discriminador (treinado), informando-nos quando o conte\u00fado audiovisual parece ser uma falsifica\u00e7\u00e3o.\u201d\n\nAlmeida ressalta que, seja qual for a tecnologia empregada, ser\u00e1 necess\u00e1rio o apoio de empresas de redes sociais, setores da sociedade e poder p\u00fablico. \u201cA sociedade tem que criar as ferramentas para se proteger disso.\u201d", "lang": "pt"}