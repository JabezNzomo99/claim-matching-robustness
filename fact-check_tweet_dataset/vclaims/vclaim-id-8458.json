{"title": "Are computers getting better than humans at reading?", "subtitle": "Computers aren't getting better than humans at reading. \u201cEven elementary school reading comprehensions are harder\u201d than the test computers passed, says the test\u2019s creator.", "author": "fullfact.org", "date": null, "vclaim_id": "vclaim-id-8458", "url": "https://fullfact.org/economy/are-computers-getting-better-humans-reading/", "vclaim": "Are computers getting better than humans at reading?\n\nThree teams of computer scientists have set new records in computer reading comprehension, achieving the highest scores ever on a standard test called \u2018SQuAD\u2019 which is designed to test reading and understanding by artificial intelligence.\n\nFor the first time these scores are better than the benchmark set by humans doing the same test.\n\nIt\u2019s a fascinating achievement but the comparison to human readers is less impressive than it seems and so is the task: one of the computer scientists behind SQuAD told the Verge, \u201ceven elementary school reading comprehensions are harder\u201d. These computers aren\u2019t going to put you out of a job\u2014yet.\n\nAs for being the first, IBM had computers beating quiz show contestants back in 2011.\n\nThis is not a test of reading as humans do it\n\nThe SQuAD test is designed to assess how well computers can process a small paragraph of text and give the correct answer to a question about it.\n\nThe exact answer to the test question is always present in the paragraph given to a computer, so it can be answered, effectively, with a single cut and paste.\n\nIt\u2019s debatable whether cutting and pasting a piece of text as an answer to a question faster and more accurately than a human does mean that a computer is reading \u2018better\u2019 than humans can.\n\nThe grand reports that computers are able to read about and understand large amounts of information to answer specific questions aren\u2019t justified.\n\nThis is not like a computer beating a grandmaster at chess\n\nThe phrasing of the claims reported by CNN might suggest SQuAD\u2019s test is a competitive task in which top human readers compete against computers in a direct challenge, similar to previous examples using popular games like Chess, Go, or Jeopardy. That\u2019s not what the test does.\n\nThe human score in SQuAD is not a benchmark of how well humans can do on the same test, it is actually there to help provide examples of questions which are poorly defined or not specific enough.\n\nAs Senior Lecturer at Bar Ilan University in Israel, Yoav Goldberg, says: \u201cSQuAD was not designed to be a realistic assessment of \u2018reading comprehension\u2019 in the popular sense [...] It was designed as a benchmark for machine learning methods, and the human evaluation was performed to assess the quality of the dataset, not the humans\u2019 abilities.\u201d\n\nIn the example below, the human score was driven down because one person answered \u201cEdinburgh\u201d as the home of Scottish Parliament, while another two people put \u201cScottish Parliament building\u201d. That shows the question was poorly defined.\n\n\n\nEven assuming that the \u201chuman score\u201d was designed as a fair representation of human reading ability, the actual human readers in SQuAD probably don\u2019t represent the best humans can do.\n\nThe people involved were encouraged to answer a question in a fairly short time frame of 24 seconds. They received a financial reward of $0.06 cents per answer.\n\nIt\u2019s fair to say that\u2019s a small reward for US and Canadian citizens working on the task, especially when compared against the 2011 Jeopardy case, where two former winners competed against a computer for a $1 million cash prize.\n\nComputers have beaten humans at quizzes before\n\nIt is a matter of debate whether, as CNN put it, this is the first time that a machine has outperformed humans on a test like this.\n\nA famous artificial intelligence reading comprehension result is the Jeopardy win by IBM Watson computer over two former human winners of the popular game show in 2011.\n\nTo answer Jeopardy questions the computer had to search through vast amounts of knowledge, including the entirety of Wikipedia. That can be seen as much harder task than selecting the best phrase out of a paragraph of 200 words for a single question in the SQuAD task.\n\n...please send us an email and we will update the factcheck.", "lang": "en"}